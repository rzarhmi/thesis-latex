
\فصل{روش‌های پیشنهادی}
%
%در این فصل نتایج جدید به‌دست‌آمده در پایان‌نامه توضیح داده می‌شود.
%در صورت نیاز می‌توان نتایج جدید را در قالب چند فصل ارائه نمود.
%همچنین در صورت وجود پیاده‌سازی، بهتر است نتایج پیاده‌سازی را 
%در فصل مستقلی پس از این فصل قرار داد.
%

\section{مقدمه}
این پروژه ادامه‌ای است بر کار \cite{st-gcn} و \cite{attention_pooling_main}. هردوی این مقالات، با استفاده از شبکه‌های عصبی سرتاسر، به بازشناسی کنش پرداخته‌اند. در این پایان‌نامه نیز سعی شده‌ است که با استفاده از روش‌های هر دوی این مقاله‌ها، درصد خطای پایین‌تری را به دست آورد. در ابتدای این فصل کلیت شبکه و برخی ریزه‌کاری‌ها که بایستی اعمال شوند توضیح داده می‌شود. در ادامه مدل توجه مورد استفاده و چگونگی کار آن شرح داده خواهد شد. در انتها نیز معیاری که برای سنجش کار استفاده خواهیم کرد معرفی می‌شود. تصویر \ref{fig:block_diagram} نموداری مختصر از کل کار انجام‌گرفته را شرح می‌دهد. 
\begin{figure}
\centerimg{block_diagram}{15cm}
\label{fig:block_diagram}
\caption[نمودار بلوکی برای شبکه‌ی ST-GCN با مدل توجه ادغامی]{نمودار بلوکی برای شبکه‌ی ST-GCN با مدل توجه ادغامی}
\end{figure}

\section{شبکه‌ی استفاده شده}
شبکه‌ی مورد استفاده در این پروژه، شبکه‌ی گراف‌-پیچشی زمان‌-مکانی (ST-GCN) است. بزرگ‌ترین تفاوت این شبکه با شبکه‌ی گراف‌-پیچشی، در نوع ورودی‌ای است که به آن داده می‌شود. ورودی st-gcn یک گراف $G=(V, E)$ است که در آن $V$ مجموعه‌ی رئوس و $E$ مجموعه‌ی یال‌های گراف است. مجموعه‌ی $V$  تعریفی مانند
\begin{equation}\label{eqn:Vset}
	V = \{v_{fi}|f = 1, ..., F , i = 1, ..., N\}
\end{equation}
 دارد. \cite{st-gcn}

\begin{figure}
\centerimg{st_gcn_example}{10cm}
\caption[مثالی از یک گراف زمان-مکانی]{{\footnotesize مثالی از یک گراف زمان-مکانی}}
\label{fig:stgExample}
\end{figure}

در رابطه‌ی \ref{eqn:Vset} ، $f$  شاخص شماره‌ی قاب\LTRfootnote{Frame} و $i$ شاخص شماره‌ی راس (مفصل) در یک قاب است. همان‌گونه که از این رابطه مشخص است، رئوس گراف تمامی مفاصل در تمامی قاب‌ها را شامل می‌شود و محدود به یک قاب نیست. به همین دلیل تعریف یال نیز بایستی شامل تمامی ارتباطات متصل کننده‌ی این رئوس باشد. \cite{st-gcn} مجموعه‌ی یال‌ها را به دو زیرمجموعه‌ی یال‌های یک قاب ($E_s$) و یال‌های بین دو قاب ($E_t$) تفکیک کرده و هرکدام را به شکل 
\begin{equation}\label{eqn:ESTset}
\begin{split}
	&E_s = \{v_{fi}v_{fj}|(i, j)\in H\} , \\
	  &E_t = \{v_{fi}v_{(f+1)i}\}
\end{split}
\end{equation}
 تعریف ‌می‌کند.\cite{st-gcn}




در رابطه‌ی \ref{eqn:ESTset} مجموعه‌ی $H$ مجموعه‌ی شامل مفاصل مجاور بدن انسان هستند. توجه کنید که چگونه مجموعه یال‌های $E_t$ مفاصل متناظر در دو قاب را به هم‌دیگر متصل می‌کند. 

برای انجام عمل پیچش در این گراف، بایستی رابطه‌ی همسایگی را برای هر راس در گراف تعریف کنیم. چرا که پارامتر وزن در عمل پیچش، به‌ازای هر راس، بر روی همسایه‌های آن راس شناور خواهد بود. برای هر راس مانند $v_{fi}$ همسایه‌های آن با رابطه‌ی 
\begin{equation}\label{eqn:Neigh}
	N(v_{fi}) = \{v_{qj}|d(v_{fj}, d_{fi})\leq K, |q-f|\leq \floor{\frac{\Gamma}{2}} \}
\end{equation}
تعریف شده‌اند.\cite{st-gcn}

در رابطه‌ی \ref{eqn:Neigh}، تابع $d$  کوتاه‌ترین مسیر بین دو راس ورودی آن را مشخص می‌کند. هم‌چنین متغیر $K$ حداکثر فاصله بین رئوس همسایه در یک قاب و متغیر $\Gamma$ حداکثر فاصله‌ بین دو راس همسایه در دوقاب مختلف را بیان می‌کند. به بیان دیگر، می‌توان گفت که $K$ اندازه‌ی ماتریس وزن در بعد مکان و $\Gamma$ اندازه‌ی آن در بعد زمان است. برای افزایش سرعت کار در این پروژه، $K = 1$ و $\Gamma = 2$ در نظر گرفته شده است. مقادیر بالاتر از این می‌تواند در کارهای آتی مورد بررسی قرار گیرد. 

شکل \ref{fig:stgExample} روابطی که تا این‌جای کار بیان شد را ترسیم کرده است. در این شکل، یال‌های یک قاب به‌صورت رنگی و یال‌های بین دو قاب بی‌رنگ هستند. هم‌چنین به‌ازای $K = 1$ و $\Gamma = 2$ رئوس $v_{11}$، $v_{12}$، $v_{21}$ و $v_{22}$ همسایه هستند. هم‌چنین دقت شود که رابطه‌ی همسایگی یک رابطه‌ی تعدی نیست.


\section{مدل توجه}
مدل توجه استفاده شده در این پروژه، الهام گرفته از مدل‌های \cite{attention_pooling_main} و \cite{attention_pooling} است. استفاده از این روش بر دو ایده‌ی کلی استوار است:
\begin{itemize}
\item اگر درست بعد از اتمام لایه‌های پیچشی، خروجی را به‌صورت یک بردار درآورده و به لایه‌ی کاملا متصل بدهیم، پارامترهای شبکه‌ بسیار زیاد شده و یادگیری را مشکل می‌کند. به همین دلیل بهتر است که قبل این کار، به‌گونه‌ای اندازه‌ی خروجی را کاهش دهیم و بعد از آن به یک لایه‌ی کاملا متصل بدهیم.\cite{attention_pooling}
\item روشی که برای کاهش اندازه اتخاذ می‌کنیم بهتر است به‌ازای ورودی‌های مختلف، پاسخ‌های متفاوتی داشته باشد. چرا که همان‌گونه که در فصل پیش به آن اشاره شد، برای برخی از ورودی‌ها احتیاجی به کل داده نیست و بهتر است پارامترها بسته به نوع ورودی، ضرایب متفاوتی داشته باشند.  
\end{itemize}

برای دستیابی به این دو مورد، \cite{attention_pooling_main} و \cite{attention_pooling} لایه‌ی ادغام توجه را معرفی کرده‌اند. بردار ساخته شده توسط این لایه با رابطه‌ی
\begin{equation}\label{eqn:APL}
	score(X) = Tr(X^{T}XW)
\end{equation}
 محاسبه می‌شود.\cite{attention_pooling_main}

در رابطه‌ی \ref{eqn:APL}، $X$  ورودی لایه‌ی ادغام به اندازه‌ی $n\times f$ و $W$ پارامتر این لایه به اندازه‌ی $f\times f$ است. برای این‌که رابطه‌ی بالا قادر به جذب خاصیت توجه باشد، کافی است پارامتر $W$ را به صورت ضرب دو بردار $f\times 1$ بنویسیم. 
\begin{equation}
	W = ab^T \rightarrow W^T = ba^T \hspace{1em} a, b \in R^{f\times 1}
\end{equation}
\begin{equation}\label{eqn:prime}
		score(X) = Tr(X^{T}Xba^T)
\end{equation}
چون برای ماتریس‌ها داریم $Tr(ABC)=Tr(CAB)$، در نتیجه رابطه‌ی \ref{eqn:prime} را می‌توان به شکل
\begin{equation}\label{eqn:sec}
	score(X) = Tr(a^TX^TXb)
\end{equation}
نوشت.

هم‌چنین چون برای یک بردار مانند $u$ داریم $Tr(u) = u$ در نتیجه رابطه‌ی \ref{eqn:sec} را نیز می‌توان به شکل
\begin{equation}\label{eqn:tri}
\begin{split}
	score(X) & = a^TX^TXb \\ 
	& = (Xa)^T(Xb)
\end{split}
\end{equation}
نوشت.\cite{attention_pooling_main}

حال بردار $score$ آماده است تا به‌عنوان ورودی به لایه‌های کاملا متصل و یا حتی به یک لایه‌ی دسته‌بند بیشینه‌ی هموار داده شود. تصویر \ref{fig:attention_pooling} جزئیات این مدل را نمایش می‌دهد.
\begin{figure}
\centerimg{attention_pooling}{15cm}
\label{fig:attention_pooling}
\caption[نموداری از مدل توجه با استفاده از لایه‌ی ادغام]{نموداری از مدل توجه با استفاده از لایه‌ی ادغام \cite{attention_pooling_main}}
\end{figure}









